{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from matplotlib import pyplot as plt\n",
    "from analysis_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### strcuture of different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_channels_en.tsv\n",
    "\n",
    "CSV file, having the following structure:\n",
    "| **category_cc** | **join_date** | **channel**              | **name_cc**             | **subscribers_cc** | **videos_cc** | **subsriber_rank_sb** | **weights** |\n",
    "|-----------------|---------------|--------------------------|-------------------------|--------------------|---------------|-----------------------|-------------|\n",
    "| News & Politics | 2013-03-11    | UCcRgZlgsk5m-aDQa_d6BTkQ | NorthWestLibertyNews... | 16700              | 845           | 639043.0              | 10.0035     |\n",
    "| Gaming          |    2012-01-15 | UCnnXR0VIJVpeL1wEr-bBaRw | Felix Guaman            | 112000             | 703           | 137318.0              | 5.4915      |\n",
    "\n",
    "With:\n",
    "- `category_cc`: most frequent category of the channel. One of: ['Gaming', 'Education', 'Entertainment', 'Howto & Style', 'Sports', 'Music', 'Film and Animation', 'Comedy', 'Nonprofits & Activism', 'People & Blogs', 'News & Politics', 'Science & Technology', 'Pets & Animals', 'Autos & Vehicles', 'Travel & Events', nan]\n",
    "- `join_date`: join date of the channel.\n",
    "- `channel`: unique channel id.\n",
    "- `name_cc`: name of the channel.\n",
    "- `subscribers_cc`: number of subscribers.\n",
    "- `videos_cc`: number of videos.\n",
    "- `subscriber_rank_sb`: rank in terms of number of subscribers.\n",
    "- `weights`: weights cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### yt_metadata_helper.feather   (yt_metadata_helper.feather.csv, filtered_yt_metadata_helper.feather.csv)\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>categories</th>\n",
    "      <th>channel_id</th>\n",
    "      <th>dislike_count</th>\n",
    "      <th>display_id</th>\n",
    "      <th>duration</th>\n",
    "      <th>like_count</th>\n",
    "      <th>upload_date</th>\n",
    "      <th>view_count</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Film &amp; Animation</td>\n",
    "      <td>UCy6sWF4taso5GtrfDGhwpBA</td>\n",
    "      <td>0.0</td>\n",
    "      <td>EXOviJ_EJDo</td>\n",
    "      <td>68</td>\n",
    "      <td>0.0</td>\n",
    "      <td>2011-12-07</td>\n",
    "      <td>76.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Gaming</td>\n",
    "      <td>UCEPYwwuGhgA9wfO2It11OXw</td>\n",
    "      <td>0.0</td>\n",
    "      <td>xSKA6VX7Tdo</td>\n",
    "      <td>125</td>\n",
    "      <td>6.0</td>\n",
    "      <td>2016-10-04</td>\n",
    "      <td>198.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>News &amp; Politics</td>\n",
    "      <td>UCojNA7ZvnmGuIvYnm44wl3Q</td>\n",
    "      <td>NaN</td>\n",
    "      <td>FsucWMijKA4</td>\n",
    "      <td>130</td>\n",
    "      <td>NaN</td>\n",
    "      <td>2010-11-18</td>\n",
    "      <td>106.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "With (values were crawled from YouTube between 2019-10-29 and 2019-11-23):\n",
    "- `categories`: category (self-defined when they upload a video to YouTube)\n",
    "- `channel_id`: unique channel id\n",
    "- `dislike_count`: dislikes of the video\n",
    "- `display_id`: unique video id\n",
    "- `duration`: duration of the video\n",
    "- `like_count`:likes of the video.\n",
    "- `upload_date`: upload date\n",
    "- `view_count`: views of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### yt_metadata.jsonl\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>categories</th>\n",
    "      <th>channel_id</th>\n",
    "      <th>crawl_date</th>\n",
    "      <th>description</th>\n",
    "      <th>dislike_count</th>\n",
    "      <th>display_id</th>\n",
    "      <th>duration</th>\n",
    "      <th>like_count</th>\n",
    "      <th>tags</th>\n",
    "      <th>title</th>\n",
    "      <th>upload_date</th>\n",
    "      <th>view_count</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Film &amp; Animation</td>\n",
    "      <td>UCy6sWF4taso5GtrfDGhwpBA</td>\n",
    "      <td>2019-10-29</td>\n",
    "      <td>description</td>\n",
    "      <td>0.0</td>\n",
    "      <td>EXOviJ_EJDo</td>\n",
    "      <td>68</td>\n",
    "      <td>0.0</td>\n",
    "      <td>tags</td>\n",
    "      <td>title</td>\n",
    "      <td>2011-12-07</td>\n",
    "      <td>76.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Gaming</td>\n",
    "      <td>UCEPYwwuGhgA9wfO2It11OXw</td>\n",
    "      <td>2019-10-29</td>\n",
    "      <td>description</td>\n",
    "      <td>0.0</td>\n",
    "      <td>xSKA6VX7Tdo</td>\n",
    "      <td>125</td>\n",
    "      <td>6.0</td>\n",
    "      <td>tags</td>\n",
    "      <td>title</td>\n",
    "      <td>2016-10-04</td>\n",
    "      <td>198.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>News &amp; Politics</td>\n",
    "      <td>UCojNA7ZvnmGuIvYnm44wl3Q</td>\n",
    "      <td>2019-10-29</td>\n",
    "      <td>description</td>\n",
    "      <td>NaN</td>\n",
    "      <td>FsucWMijKA4</td>\n",
    "      <td>130</td>\n",
    "      <td>NaN</td>\n",
    "      <td>tags</td>\n",
    "      <td>title</td>\n",
    "      <td>2010-11-18</td>\n",
    "      <td>106.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "With (values were crawled from YouTube between 2019-10-29 and 2019-11-23):\n",
    "- `categories`: category (self-defined when they upload a video to YouTube)\n",
    "- `channel_id`: unique channel id\n",
    "- `dislike_count`: dislikes of the video\n",
    "- `display_id`: unique video id\n",
    "- `duration`: duration of the video\n",
    "- `like_count`:likes of the video.\n",
    "- `upload_date`: upload date\n",
    "- `view_count`: views of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_timeseries_en.tsv\n",
    "\n",
    "| **channel**              | **category**       | **datetime** | **views**   | **delta_views** | **subs** | **delta_subs** | **videos** | **delta_videos** | **activity** |\n",
    "|--------------------------|--------------------|--------------|-------------|-----------------|----------|----------------|------------|------------------|--------------|\n",
    "| UCBJuEqXfXTdcPSbGO9qqn1g | Film and Animation | 2017-07-03 | 202495  |           0 |  650 |   0        |      5 |            0 |        3 |\n",
    "| UCBJuEqXfXTdcPSbGO9qqn1g | Film and Animation | 2017-07-10 | 394086  |      191591 | 1046 | 396        |      6 |            1 |        1 |\n",
    "\n",
    "With:\n",
    "- `channel`: channel id.\n",
    "- `category`: category of the channel as assigned by `socialblade.com` according to the last 10 videos at time of crawl.\n",
    "- `datetime`: Week related to the data point.\n",
    "- `views`: Total number of views the channel had this week.\n",
    "- `delta_views`: Delta views obtained this week.\n",
    "- `subs`: Total number of subscribers the channel had this week.\n",
    "- `delta_subs`: Delta subscribers obtained this week.\n",
    "- `videos`: Total number of videos the channel had this week.\n",
    "- `delta_videos`: Delta videos obtained this week.\n",
    "- `activity`: Number of videos published in the last 15 days.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### youtube_comments.tsv\n",
    "\n",
    "| **author** | **video_id**      |  **likes** |  **replies** |\n",
    "|------|--------------|-------|---------|\n",
    "| 1      | Gkb1QMHrGvA   |  2     |  0       |\n",
    "| 1      | CNtp0xqoods   |  0     |  0       |\n",
    "| 1      | 249EEzQmVmQ   |  1     |  0       |\n",
    "\n",
    "With (data obtained at crawl time between 2019-09-12 and 2019-09-17):\n",
    "- `author`: anonymized author id (unique)\n",
    "- `video_id`: unique video id of the video the comment was written\n",
    "- `likes`: likes for the comment\n",
    "- `replies`: replies for the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels_df_f = pd.read_csv(\"./Youniverse/channels_dfannels_en.tsv\", sep=\"\\t\")\n",
    "\n",
    "pl_df_f = pl.read_csv(\"../Youniverse/df_channels_en.tsv\", separator=\"\\t\")\n",
    "\n",
    "filtered_df_ch = pl_df_f.filter(pl.col(\"category_cc\") == \"News & Politics\")\n",
    "pl_df_f.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vd_f = pd.read_feather(\"../Youniverse/yt_metadata_helper.feather\")\n",
    "\n",
    "# df_vd_f.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "# df_vd_f.to_csv(\"../Youniverse/yt_metadata_helper.feather.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_df.filter(pl.col(\"channel\").is_in(filtered_df_ch[\"channel\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yt metadata in chunks and filter for videos contained in filtered_df_ch\n",
    "# df.filter(pl.col(\"categories\") == \"News & Politics\") to filter for categories of videos instead\n",
    "reader = pl.read_csv_batched(\n",
    "    \"../Youniverse/yt_metadata_helper.feather.csv\",\n",
    "    separator=\"\\t\",\n",
    "    batch_size=5000\n",
    ")\n",
    "\n",
    "\n",
    "batches = reader.next_batches(5)  \n",
    "i = 0\n",
    "while batches:\n",
    "    for df in batches:\n",
    "        if i == 0:\n",
    "            df.filter(pl.col(\"channel_id\").is_in(filtered_df_ch[\"channel\"])).write_csv(\"../Youniverse/filtered_yt_metadata_helper.feather.csv\", include_header=True)\n",
    "        else:\n",
    "            with open(\"../Youniverse/filtered_yt_metadata_helper.feather.csv\", \"a\") as fh:\n",
    "                fh.write(df.filter(pl.col(\"channel_id\").is_in(filtered_df_ch[\"channel\"])).write_csv(file=None, include_header=False))\n",
    "        i = i+1\n",
    "        print(f\"batch {i}\\r\", end='')\n",
    "    batches = reader.next_batches(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_metadata_feather = pl.read_csv(\"../Youniverse/filtered_yt_metadata_helper.feather.csv\")\n",
    "filtered_df_metadata_feather.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df_metadata_feather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filtered_df_metadata_feather[\"categories\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_metadata_feather[\"categories\"].value_counts().sort(by=\"count\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_json(\"../Youniverse/yt_metadata_en.jsonl\", lines=True, chunksize = 500)\n",
    "for i, c in enumerate(chunks):\n",
    "    c = c[c[\"channel_id\"].isin(filtered_df_ch[\"channel\"])]\n",
    "    if i == 0:\n",
    "        print(c)\n",
    "        c.to_csv(\"../Youniverse/filtered_yt_metadata.csv\", header=True, index=False)\n",
    "    else: \n",
    "        with open(\"../Youniverse/filtered_yt_metadata.csv\", \"a\") as fh:\n",
    "            fh.write(c.to_csv(path_or_buf=None, header=False, index=False))\n",
    "    print(f\"batch {i} / 145390 \\r\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yt metadata in chunks and filter for videos contained in filtered_df_ch\n",
    "# df.filter(pl.col(\"categories\") == \"News & Politics\") to filter for categories of videos instead\n",
    "batch_size = 10000\n",
    "reader = pl.read_csv_batched(\n",
    "    \"../Youniverse/youtube_comments.tsv\",\n",
    "    separator=\"\\t\",\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "total_batches = 8600000000/batch_size\n",
    "\n",
    "batches = reader.next_batches(5)  \n",
    "i = 0\n",
    "while batches:\n",
    "    for df in batches:\n",
    "        if i == 0:\n",
    "            # df.filter(pl.col(\"video_id\").is_in(filtered_df_metadata_feather[\"display_id\"])).write_csv(\"../Youniverse/filtered_youtube_comments.tsv\", include_header=True)\n",
    "            df.filter(pl.col(\"video_id\") == \"fXN0ABkfZ7M\").write_csv(\"../Youniverse/filtered_youtube_comments.tsv\", include_header=True)\n",
    "        else:\n",
    "            with open(\"../Youniverse/filtered_youtube_comments.tsv\", \"a\") as fh:\n",
    "                fh.write(df.filter(pl.col(\"video_id\") == \"fXN0ABkfZ7M\").write_csv(file=None, include_header=False))\n",
    "        i = i+1\n",
    "        print(f\"batch {i} / {total_batches} \\r\", end='')\n",
    "    batches = reader.next_batches(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yt metadata in chunks and filter for videos contained in filtered_df_ch\n",
    "reader = pl.read_csv_batched(\n",
    "    \"../Youniverse/num_comments_authors.tsv\",\n",
    "    separator=\"\\t\",\n",
    "    batch_size=5000\n",
    ")  \n",
    "batches = reader.next_batches(5)  \n",
    "for df in batches:  \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pl_df_f[\"category_cc\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries = pl.read_csv(\"../Youniverse/df_timeseries_en.tsv\", separator=\"\\t\")\n",
    "df_timeseries.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries.filter(pl.col(\"channel\").is_in(filtered_df_ch[\"channel\"])).write_csv(\"../Youniverse/filtered_df_timeseries_en.tsv\", include_header=True, separator=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_timeseries = pl.read_csv(\"../Youniverse/filtered_df_timeseries_en.tsv\", separator=\"\\t\")\n",
    "filtered_df_timeseries.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = filtered_df_timeseries.group_by('channel').agg(pl.col('activity').mean().alias('mean_activity'))\n",
    "\n",
    "# Extract the mean activity values into a list\n",
    "mean_activities = grouped_df['mean_activity'].to_list()\n",
    "\n",
    "# Plot histogram of the mean activity values\n",
    "plt.hist(mean_activities, bins=100, edgecolor=\"black\", alpha=0.7)\n",
    "plt.xlabel('Mean Activity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Mean Activity by Channel')\n",
    "# plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 56 == 4 videos per day\n",
    "len(grouped_df.filter(pl.col(\"mean_activity\")>56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_activity_channels = channels_df.filter(pl.col(\"channel\").is_in(grouped_df.filter(pl.col(\"mean_activity\")>56)[\"channel\"]))\n",
    "# merge high_activity_channels with grouped_df on the channel column\n",
    "high_activity_channels = high_activity_channels.join(grouped_df, on=\"channel\", how=\"inner\")\n",
    "high_activity_channels.sort(by=\"mean_activity\", descending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the high_activity_channels to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_polars_to_csv(polars_dataframe, name):\n",
    "# # Convert to pandas DataFrame\n",
    "    polars_dataframe = polars_dataframe.to_pandas()\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    polars_dataframe.to_csv(f\"{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keys\n",
    "YOUTUBE_KEY = keys.YOUTUBE_API_KEY\n",
    "OPEN_API_KEY = keys.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get country of channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_KEY)\n",
    "\n",
    "def get_channel_country(channel_id):\n",
    "    # Make the API request to get the channel details\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    \n",
    "    # Execute the request and get the response\n",
    "    response = request.execute()\n",
    "    \n",
    "    # Check if the response contains the necessary information\n",
    "    if \"items\" in response and len(response[\"items\"]) > 0:\n",
    "        # Extract country information from the channel snippet\n",
    "        country = response[\"items\"][0][\"snippet\"].get(\"country\", \"Country not available\")\n",
    "        return country\n",
    "    else:\n",
    "        return \"Channel not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_activity_channels = high_activity_channels.with_columns(\n",
    "    pl.col(\"channel\").map_elements(lambda channel_id:get_channel_country(channel_id)).alias(\"Channel_country\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(high_activity_channels.sample(100))\n",
    "write_polars_to_csv(high_activity_channels, \"high_activity_channels_with_country_test_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out english speaking channels with CHATGPT LLM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chunk reader\n",
    "chunk_reader = pd.read_csv(\"../Youniverse/filtered_yt_metadata.csv\", chunksize=5000)\n",
    "\n",
    "# Read the first chunk and print the first few rows\n",
    "chunk = next(chunk_reader)  # Get the first chunk\n",
    "print(chunk.head())  # Display the first few rows of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second dataset (with the list of channel IDs to compare against)\n",
    "channels_df = pd.read_csv(\"high_activity_channels_with_country.csv\") \n",
    "channel_ids = set(channels_df['channel'].unique())  \n",
    "# Initialize the chunk reader for the large CSV file\n",
    "chunk_reader = pd.read_csv(\"../Youniverse/filtered_yt_metadata.csv\", chunksize=5000)\n",
    "\n",
    "matching_videos = []\n",
    "# Dictionary to track how many videos are saved for each channel\n",
    "channel_video_count = {channel_id: 0 for channel_id in channel_ids}\n",
    "\n",
    "for chunk in chunk_reader:\n",
    "    # Filter rows where channel_id in chunk is in the set of channel_ids from channels_df\n",
    "    matching_rows = chunk[chunk['channel_id'].isin(channel_ids)]\n",
    "    for channel_id, group in matching_rows.groupby('channel_id'):\n",
    "        # If we've already saved 5 videos for this channel, skip it\n",
    "        if channel_video_count[channel_id] >= 5:\n",
    "            continue\n",
    "        # Get the first 5 videos for this channel (or fewer if there are less than 5)\n",
    "        first_5_videos = group.head(5 - channel_video_count[channel_id])  # Adjust to avoid exceeding 5\n",
    "        # Add the number of videos saved for this channel\n",
    "        channel_video_count[channel_id] += len(first_5_videos)\n",
    "        matching_videos.append(first_5_videos)\n",
    "\n",
    "# Convert the list into DataFrame\n",
    "final_df = pd.concat(matching_videos, ignore_index=True)\n",
    "final_df.to_csv('matching_videos.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv(\"./data/matching_videos.csv\")\n",
    "print(file.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "import ollama\n",
    "import datetime\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "client = OpenAI(api_key=OPEN_API_KEY)\n",
    "\n",
    "# YouTube API credentials and setup\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_KEY)\n",
    "\n",
    "# Define the function to detect the language using ChatGPT\n",
    "def check_video_language(video_title, video_description, closed_captions=\"\"):\n",
    "    # Combine title, description, and captions to form the text to be checked\n",
    "    print(\"title: \", video_title)\n",
    "    print(\"description: \", video_description)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who only focuses on language identification.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Given the title and description of a YouTube video, please determine if the text is in English. Ignore URLs and non-English symbols. \n",
    "        Respond with \"yes\" if you think the text is in English, and \"no\" if you think it is not.\n",
    "\n",
    "        Title: \"{video_title}\"\n",
    "        Description: \"{video_description}\"\n",
    "\n",
    "        Is the text in English?\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.2,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "\n",
    "    print(\"Chat response: \", response)\n",
    "    # Check if the response includes \"yes\"\n",
    "    return \"yes\" in response\n",
    "\n",
    "def check_channel_english(channel_id):\n",
    "    videos = final_df.loc[final_df['channel_id'] == channel_id]\n",
    "    # print(videos)\n",
    "    for index, video in videos.iterrows():\n",
    "        # Check if the text is in English using CHATGPT API\n",
    "        is_english = check_video_language(video_title=video['title'], video_description=video['description'])\n",
    "        if not is_english:\n",
    "            print(\"channel is not english\")\n",
    "            return False  # If any video is not English, return False\n",
    "        time.sleep(0.5)\n",
    "    print(\"channel is english\")\n",
    "    return True  # If all videos checked are English, return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING \n",
    "\n",
    "check_channel_english(\"UClMs26ViHFMy7MS897Alcxw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through pandas dataframe and check if channel is english\n",
    "# Check each channel and store results\n",
    "high_activity_channels = high_activity_channels.with_columns(\n",
    "    pl.col(\"channel\").map_elements(lambda channel_id:check_channel_english(channel_id)).alias(\"Is_English\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_polars_to_csv(high_activity_channels, \"high_activity_channels_country_and_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = pd.read_csv(\"data\\high_activity_channels_country_and_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered[\"Is_English\"].value_counts()\n",
    "english = filtered[filtered[\"Is_English\"] == True]\n",
    "print(len(english))\n",
    "print(\"english\")\n",
    "print(english[\"Channel_country\"].value_counts())\n",
    "\n",
    "not_english = filtered[filtered[\"Is_English\"] == False]\n",
    "print(\"not_english\")\n",
    "print(not_english[\"Channel_country\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_df = pd.read_csv(\"./data/high_activity_channels_country_and_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\AppData\\Local\\Temp\\ipykernel_10736\\3579991826.py:1: DtypeWarning: Columns (4,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  videos_df = pd.read_csv(\"./../data/filtered_yt_metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "videos_df = pd.read_csv(\"./../data/filtered_yt_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>description</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>display_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:47:10.520209</td>\n",
       "      <td>ðŸ‘• Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>195.0</td>\n",
       "      <td>MBgzne7djFU</td>\n",
       "      <td>378</td>\n",
       "      <td>47027.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>Elizabeth Warren Gets a Big Surprise at the Ai...</td>\n",
       "      <td>2019-10-03 00:00:00</td>\n",
       "      <td>374711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:46:16.481889</td>\n",
       "      <td>ðŸ‘• Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>AbH3pJnFgY8</td>\n",
       "      <td>278</td>\n",
       "      <td>36384.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>No More Twitter? ðŸ˜‚</td>\n",
       "      <td>2019-10-02 00:00:00</td>\n",
       "      <td>245617.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:46:17.137786</td>\n",
       "      <td>ðŸ‘• Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>143.0</td>\n",
       "      <td>QBuwj_h1SH4</td>\n",
       "      <td>385</td>\n",
       "      <td>40597.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>The Only Thing Stopping Them ðŸ˜‚</td>\n",
       "      <td>2019-10-01 00:00:00</td>\n",
       "      <td>299535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:46:17.823119</td>\n",
       "      <td>ðŸ‘• Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Reogq26-KpI</td>\n",
       "      <td>419</td>\n",
       "      <td>42658.0</td>\n",
       "      <td>Funny,Entertainment,Fun,Laughing,Educational,L...</td>\n",
       "      <td>Speaking of Losers...</td>\n",
       "      <td>2019-09-30 00:00:00</td>\n",
       "      <td>357126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCzUV5283-l5c0oKRtyenj6Q</td>\n",
       "      <td>2019-11-22 08:46:18.497042</td>\n",
       "      <td>ðŸ‘• Order your shirts here: https://Teespring.co...</td>\n",
       "      <td>136.0</td>\n",
       "      <td>uBY9OtlSnX8</td>\n",
       "      <td>414</td>\n",
       "      <td>44246.0</td>\n",
       "      <td>Funny,Entertainment,Laughing,Educational,Learn...</td>\n",
       "      <td>The Circus Continues!</td>\n",
       "      <td>2019-09-27 00:00:00</td>\n",
       "      <td>297704.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503605</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrwE8kVqtIUVUzKui2WVpuQ</td>\n",
       "      <td>2019-11-01 23:46:02.294620</td>\n",
       "      <td>Shri Manoj Kumar Tiwari's speech during Motion...</td>\n",
       "      <td>3</td>\n",
       "      <td>YQLoxwLpjSU</td>\n",
       "      <td>270</td>\n",
       "      <td>67</td>\n",
       "      <td>BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...</td>\n",
       "      <td>Shri Manoj Kumar Tiwari's speech during Motion...</td>\n",
       "      <td>2017-02-06 00:00:00</td>\n",
       "      <td>4409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503606</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrwE8kVqtIUVUzKui2WVpuQ</td>\n",
       "      <td>2019-11-01 23:46:06.401481</td>\n",
       "      <td>Shri La Ganesan's speech during Motion of Than...</td>\n",
       "      <td>0</td>\n",
       "      <td>mINQHg1QBcg</td>\n",
       "      <td>878</td>\n",
       "      <td>21</td>\n",
       "      <td>BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...</td>\n",
       "      <td>Shri La Ganesan's speech during Motion of Than...</td>\n",
       "      <td>2017-02-06 00:00:00</td>\n",
       "      <td>1172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503607</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrwE8kVqtIUVUzKui2WVpuQ</td>\n",
       "      <td>2019-11-01 23:46:09.530822</td>\n",
       "      <td>Shri Mukhtar Abbas Naqvi's speech during Motio...</td>\n",
       "      <td>2</td>\n",
       "      <td>x20aNOWh1yI</td>\n",
       "      <td>1003</td>\n",
       "      <td>35</td>\n",
       "      <td>BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...</td>\n",
       "      <td>Shri Mukhtar Abbas Naqvi's speech during Motio...</td>\n",
       "      <td>2017-02-06 00:00:00</td>\n",
       "      <td>1898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503608</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrwE8kVqtIUVUzKui2WVpuQ</td>\n",
       "      <td>2019-11-01 23:46:00.080054</td>\n",
       "      <td>BJP submitted complaint to EC against Chief Se...</td>\n",
       "      <td>0</td>\n",
       "      <td>-Nn6FL2gqEw</td>\n",
       "      <td>755</td>\n",
       "      <td>27</td>\n",
       "      <td>BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...</td>\n",
       "      <td>BJP submitted complaint to EC against Chief Se...</td>\n",
       "      <td>2017-02-06 00:00:00</td>\n",
       "      <td>726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503609</th>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>UCrwE8kVqtIUVUzKui2WVpuQ</td>\n",
       "      <td>2019-11-01 23:46:02.381286</td>\n",
       "      <td>Shri Amit Shah speech at public meeting in Noi...</td>\n",
       "      <td>31</td>\n",
       "      <td>7gxEjSoRVgA</td>\n",
       "      <td>1771</td>\n",
       "      <td>327</td>\n",
       "      <td>BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...</td>\n",
       "      <td>Shri Amit Shah speech at public meeting in Noi...</td>\n",
       "      <td>2017-02-06 00:00:00</td>\n",
       "      <td>37572.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9503610 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              categories                channel_id  \\\n",
       "0        News & Politics  UCzUV5283-l5c0oKRtyenj6Q   \n",
       "1        News & Politics  UCzUV5283-l5c0oKRtyenj6Q   \n",
       "2        News & Politics  UCzUV5283-l5c0oKRtyenj6Q   \n",
       "3        News & Politics  UCzUV5283-l5c0oKRtyenj6Q   \n",
       "4        News & Politics  UCzUV5283-l5c0oKRtyenj6Q   \n",
       "...                  ...                       ...   \n",
       "9503605  News & Politics  UCrwE8kVqtIUVUzKui2WVpuQ   \n",
       "9503606  News & Politics  UCrwE8kVqtIUVUzKui2WVpuQ   \n",
       "9503607  News & Politics  UCrwE8kVqtIUVUzKui2WVpuQ   \n",
       "9503608  News & Politics  UCrwE8kVqtIUVUzKui2WVpuQ   \n",
       "9503609  News & Politics  UCrwE8kVqtIUVUzKui2WVpuQ   \n",
       "\n",
       "                         crawl_date  \\\n",
       "0        2019-11-22 08:47:10.520209   \n",
       "1        2019-11-22 08:46:16.481889   \n",
       "2        2019-11-22 08:46:17.137786   \n",
       "3        2019-11-22 08:46:17.823119   \n",
       "4        2019-11-22 08:46:18.497042   \n",
       "...                             ...   \n",
       "9503605  2019-11-01 23:46:02.294620   \n",
       "9503606  2019-11-01 23:46:06.401481   \n",
       "9503607  2019-11-01 23:46:09.530822   \n",
       "9503608  2019-11-01 23:46:00.080054   \n",
       "9503609  2019-11-01 23:46:02.381286   \n",
       "\n",
       "                                               description dislike_count  \\\n",
       "0        ðŸ‘• Order your shirts here: https://Teespring.co...         195.0   \n",
       "1        ðŸ‘• Order your shirts here: https://Teespring.co...         114.0   \n",
       "2        ðŸ‘• Order your shirts here: https://Teespring.co...         143.0   \n",
       "3        ðŸ‘• Order your shirts here: https://Teespring.co...         193.0   \n",
       "4        ðŸ‘• Order your shirts here: https://Teespring.co...         136.0   \n",
       "...                                                    ...           ...   \n",
       "9503605  Shri Manoj Kumar Tiwari's speech during Motion...             3   \n",
       "9503606  Shri La Ganesan's speech during Motion of Than...             0   \n",
       "9503607  Shri Mukhtar Abbas Naqvi's speech during Motio...             2   \n",
       "9503608  BJP submitted complaint to EC against Chief Se...             0   \n",
       "9503609  Shri Amit Shah speech at public meeting in Noi...            31   \n",
       "\n",
       "          display_id duration like_count  \\\n",
       "0        MBgzne7djFU      378    47027.0   \n",
       "1        AbH3pJnFgY8      278    36384.0   \n",
       "2        QBuwj_h1SH4      385    40597.0   \n",
       "3        Reogq26-KpI      419    42658.0   \n",
       "4        uBY9OtlSnX8      414    44246.0   \n",
       "...              ...      ...        ...   \n",
       "9503605  YQLoxwLpjSU      270         67   \n",
       "9503606  mINQHg1QBcg      878         21   \n",
       "9503607  x20aNOWh1yI     1003         35   \n",
       "9503608  -Nn6FL2gqEw      755         27   \n",
       "9503609  7gxEjSoRVgA     1771        327   \n",
       "\n",
       "                                                      tags  \\\n",
       "0        Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "1        Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "2        Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "3        Funny,Entertainment,Fun,Laughing,Educational,L...   \n",
       "4        Funny,Entertainment,Laughing,Educational,Learn...   \n",
       "...                                                    ...   \n",
       "9503605  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...   \n",
       "9503606  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...   \n",
       "9503607  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...   \n",
       "9503608  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...   \n",
       "9503609  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...   \n",
       "\n",
       "                                                     title  \\\n",
       "0        Elizabeth Warren Gets a Big Surprise at the Ai...   \n",
       "1                                       No More Twitter? ðŸ˜‚   \n",
       "2                           The Only Thing Stopping Them ðŸ˜‚   \n",
       "3                                    Speaking of Losers...   \n",
       "4                                    The Circus Continues!   \n",
       "...                                                    ...   \n",
       "9503605  Shri Manoj Kumar Tiwari's speech during Motion...   \n",
       "9503606  Shri La Ganesan's speech during Motion of Than...   \n",
       "9503607  Shri Mukhtar Abbas Naqvi's speech during Motio...   \n",
       "9503608  BJP submitted complaint to EC against Chief Se...   \n",
       "9503609  Shri Amit Shah speech at public meeting in Noi...   \n",
       "\n",
       "                 upload_date  view_count  \n",
       "0        2019-10-03 00:00:00    374711.0  \n",
       "1        2019-10-02 00:00:00    245617.0  \n",
       "2        2019-10-01 00:00:00    299535.0  \n",
       "3        2019-09-30 00:00:00    357126.0  \n",
       "4        2019-09-27 00:00:00    297704.0  \n",
       "...                      ...         ...  \n",
       "9503605  2017-02-06 00:00:00      4409.0  \n",
       "9503606  2017-02-06 00:00:00      1172.0  \n",
       "9503607  2017-02-06 00:00:00      1898.0  \n",
       "9503608  2017-02-06 00:00:00       726.0  \n",
       "9503609  2017-02-06 00:00:00     37572.0  \n",
       "\n",
       "[9503610 rows x 12 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_channels = []\n",
    "event_videos = []\n",
    "event_timeseries = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out a certain event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all dataframes\n",
    "timeseries_df = pl.read_csv(\"./../data/filtered_df_timeseries_en.tsv\", separator='\\t')\n",
    "num_comments = pl.read_csv(\"./../data/num_comments.tsv\", separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join videos dataset with num comment dataset for easier use\n",
    "videos_df = videos_df.join(num_comments, on='display_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename channel_id columns to all have the same name\n",
    "channels_df = channels_df.rename({'channel':'channel_id'})\n",
    "timeseries_df = timeseries_df.rename({'channel':'channel_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter by date\n",
    "min_date = pl.datetime(2017,1,1)\n",
    "max_date = pl.datetime(2018,1,1)\n",
    "\n",
    "timeseries_df = timeseries_df.with_columns(pl.col('datetime').str.to_datetime())\n",
    "videos_df = videos_df.with_columns(pl.col('upload_date').str.to_datetime())\n",
    "\n",
    "timeseries_df = timeseries_df.filter((pl.col('datetime') >= min_date) & (pl.col('datetime') <= max_date))\n",
    "videos_df = videos_df.filter((pl.col('upload_date') >= min_date) & (pl.col('upload_date') <= max_date))\n",
    "channels_df = channels_df.filter(pl.col('channel_id').is_in(videos_df['channel_id']))\n",
    "videos_df = videos_df.join(num_comments, on='display_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### !!! Note : Keyword filtering needs to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionnary to more easily navigate between channel name and channel id\n",
    "channel_dict = dict(channels_df[['name_cc','channel_id']].iter_rows())\n",
    "inv_channel_dict = {v: k for k, v in channel_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify holes in the data\n",
    "    - channels that donâ€™t report for specific events\n",
    "    - videos with too few comments (under 50 it is not in the comment dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data from next event\n",
    "event_timeseries.append(timeseries_df)\n",
    "event_videos.append(videos_df)\n",
    "event_channels.append(channels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for channels that do not report on certain events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels that report on event1 but not on event2\n",
    "event_channels[0].filter(~pl.col('channel_id').is_in(event_channels[1]['channel_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels that report on event2 but not on event1\n",
    "event_channels[1].filter(~pl.col('channel_id').is_in(event_channels[0]['channel_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating videos with too few comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the videos with too few comments can be excluded by filtering\n",
    "comment_threshold = 100\n",
    "\n",
    "too_few_comms = videos_df.filter(pl.col('num_comms') < comment_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare channels\n",
    "    - this channels videos, have these characteristics, or perform well with these subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channels with correlated video performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get general statistics for all channel\n",
    "#gives information on the general performance characteristics of the videos from each channel\n",
    "vid_count, vid_mean, vid_std, vid_med = get_general_ch_statistics(videos_df, cols_to_keep=['dislike_count','like_count','view_count','num_comms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_mean_performance = vid_mean.drop('duration')\n",
    "cov = plot_covariance (vid_mean_performance,'Covariance matrix between channels', 'Histogram of covariances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e10\n",
    "corrolated_channels = get_correlated_channels(vid_mean_performance,threshold)\n",
    "corrolated_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channels with correlated video characteristics (length, key words?, ...)\n",
    "\n",
    "##### !!! Note : Keyword analysis needs to be added for fully functionning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_mean_characteristics = vid_mean.drop(['num_comms','like_count','dislike_count','view_count'])\n",
    "cov = plot_covariance(vid_mean_characteristics, 'Covariance matrix of video characteristics', 'Histogram of covariances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print channels that have similar videos\n",
    "threshold = 1e10\n",
    "corrolated_channels = get_correlated_channels(vid_mean_characteristics, threshold)\n",
    "corrolated_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More in depth comparaison between two given channels\n",
    "\n",
    "Optional procedure to analyse more in depth the relation between two channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on video dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest : checks the null hypothesis that two independant channels have an identical mean number of views, likes etc...\n",
    "# used to compare if two sample's means differ significantly or not\n",
    "\n",
    "ttest_between_two_channels(videos_df, channel_dict['BBC News'],channel_dict[\"CNBC\"], 'num_comms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F test : test for the null hypothesis that two channels have the same variance\n",
    "# used to compare if two sample's variance differ significantly or not\n",
    "\n",
    "Ftest_between_two_channels(videos_df, channel_dict['BBC News'],channel_dict[\"CNBC\"], 'num_comms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on timeseries dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_count, ts_mean, ts_std, ts_med = get_general_ch_statistics(timeseries_df,cols_to_keep=['views', 'delta_views', 'subs','delta_subs','videos','delta_videos','activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_between_two_channels(timeseries_df, channel_dict['BBC News'],channel_dict[\"CNBC\"], 'activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ftest_between_two_channels(timeseries_df, channel_dict['BBC News'],channel_dict[\"CNBC\"], 'activity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare channel's video performance when normalized by size (number of subscribers or number of views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize by subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_videos_df = normalize_vids_with_timeseries(videos_df, timeseries_df, 'subs') #not working because of video_df issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['dislike_count','like_count','view_count','num_comms','duration']\n",
    "vid_mean, vid_std, vid_med = get_general_ch_statistics(normalized_videos_df, cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = plot_covariance (vid_mean.drop('duration'),'Covariance matrix between channels normalized by subscribers', 'Histogram of covariances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 25000\n",
    "corrolated_channels = get_correlated_channels(vid_mean,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize by views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_videos_df = normalize_vids_with_timeseries(videos_df, timeseries_df, 'views') #not working because of video_df issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['dislike_count','like_count','view_count','num_comms','duration']\n",
    "vid_mean, vid_std, vid_med = get_general_ch_statistics(normalized_videos_df, cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = plot_covariance (vid_mean.drop('duration'),'Covariance matrix between channels normalized by subscribers', 'Histogram of covariances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 25000\n",
    "corrolated_channels = get_correlated_channels(vid_mean,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare channel performance across events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_event_1 = event_videos[0]\n",
    "videos_event_2 = event_videos[1]\n",
    "\n",
    "ts_event_1 = event_timeseries[0]\n",
    "ts_event_2 = event_timeseries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate general statistics for both events\n",
    "vid_count_1, vid_mean_1, vid_std_1, vid_med_1 = get_general_ch_statistics(videos_event_1, cols_to_keep=['dislike_count','like_count','view_count','num_comms'])\n",
    "\n",
    "vid_count_2, vid_mean_2, vid_std_2, vid_med_2 = get_general_ch_statistics(videos_event_2, cols_to_keep=['dislike_count','like_count','view_count','num_comms'])\n",
    "\n",
    "\n",
    "ts_count_1, ts_mean_1, ts_std_1, ts_med_1 = get_general_ch_statistics(ts_event_1,cols_to_keep=['views', 'delta_views', 'subs','delta_subs','videos','delta_videos','activity'])\n",
    "ts_count_2, ts_mean_2, ts_std_2, ts_med_2 = get_general_ch_statistics(ts_event_2,cols_to_keep=['views', 'delta_views', 'subs','delta_subs','videos','delta_videos','activity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare general channel performance between multiple events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_performance = pl.concat([vid_mean_1.mean(),vid_mean_2.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covariance matrix for the channel performance to identify channels that perform similarly for a given event.\n",
    "cov = plot_covariance(event_performance,'Covariance across the mean performance of all channels for different events','Histogram of the covariance between events')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare a given channel statistic between two events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest : checks the null hypothesis that a given parameter has the same mean across between two events\n",
    "# used to compare if two means differ significantly or not\n",
    "\n",
    "ttest_between_events(ts_mean_1['activity'], ts_mean_2['activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ftest : checks the null hypothesis that a given parameter has the same variance across between two events\n",
    "# used to compare if two means differ significantly or not\n",
    "\n",
    "Ftest_between_events(ts_mean_1['activity'], ts_mean_2['activity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare between kinds of events and where events are from\n",
    "    - how many videos\n",
    "    - how many views\n",
    "    - interactions: likes, comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare number of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare number of videos between two events\n",
    "\n",
    "compare_overall_vid_count_between_events(vid_count_1, vid_count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average number of videos per channel between two events\n",
    "ttest_between_events(vid_count_1['counts'], vid_count_2['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare variance of the number of videos per channel between two events\n",
    "Ftest_between_events(vid_count_1['counts'], vid_count_2['counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse each event by videos (number of views, number of likes/dislikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_means_1,v_stdevs_1,v_medians_1 = get_general_vid_statistics(videos_event_1)\n",
    "v_means_2,v_stdevs_2,v_medians_2 = get_general_vid_statistics(videos_event_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.concat([v_means_1,v_means_2]).insert_column(0,pl.Series(['event_1','event_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_video_statistics_between_events(videos_event_1,videos_event_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_between_events(videos_event_1['view_count'], videos_event_2['view_count'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
